# Configmaps

## Using helm

I have created a configmap template, and added it as a volume mount to deployment.
Sometimes it is better to put such things in a values, to make them optional:
write loop over some array of volumes defined in values. However, it is faster
and easier to hard-code it, if we do not plan to reuse chart for other projects :)

Output of the cat command is the following:

```sh
$ kubectl -n testk8s exec python-app-66bd4b7dc8-bb85r -- cat config.json | jq .
[
  {
    "postId": 1,
    "id": 1,
    "name": "id labore ex et quam laborum",
    "email": "Eliseo@gardner.biz",
    "body": "laudantium enim quasi est quidem magnam voluptate ipsam eos\n
    tempora quo necessitatibus\ndolor quam autem quasi\nreiciendis et nam
    sapiente accusantium"
  }
]
```

## Bonus

1. Affinity - some rules, according to which pods should run on the same node.

1. Anti-affinity - some rules, according to which pods should not run on the
    same node.

1. Headless service - usual service is a load balancer with its own IP, which
    routes requests to one of it's backend pod. However, sometimes we want to know
    exact IPs of pods. Dns lookup for a headless svc will return all pods address
    instead of svc own address.

1. Taint - kind of a key-value label of a node, that are accessed by scheduler.
    Example is  `node-role.kubernetes.io/master:NoSchedule` - we do not want to
    schedule our pods on master node.

1. Tolerations - Some pods should still be scheduled on tainted nodes. For
    example, we have node with GPUs, and we put taint on it, because we do not want
    to schedule some general pods on it. However, pods which need this GPU will
    have a toleration to this taint and can be scheduled here

1. Node selector - again, consider example with GPU. Now scheduler can schedule
    our GPU pod to a GPU node, but it can also schedule it to any other node in
    cluster pools. To prevent this, we annotate GPU node somehow and then write
    a selector based on these annotations, so that GPU podes will be scheduled
    **only** to GPU nodes.
