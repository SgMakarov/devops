# StatefulSets

## Output

```sh
$ kubectl get po,sts,svc,pvc -n k8stest
NAME               READY   STATUS    RESTARTS   AGE
pod/python-app-0   1/1     Running   0          10m
pod/python-app-1   1/1     Running   0          10m
pod/python-app-2   1/1     Running   0          10m

NAME                          READY   AGE
statefulset.apps/python-app   3/3     10m

NAME                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/python-app   LoadBalancer   10.98.10.206   <pending>     5000:31278/TCP   10m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/python-app-visited-mount-python-app-0   Bound    pvc-82b4e3c9-7f00-40cc-b4e6-8f96d8461dc2   1Gi        RWO            standard       10m
persistentvolumeclaim/python-app-visited-mount-python-app-1   Bound    pvc-69ac8af7-fdf9-4f5e-9b96-8f59318cdaa3   1Gi        RWO            standard       10m
persistentvolumeclaim/python-app-visited-mount-python-app-2   Bound    pvc-07c3f290-f57b-4b29-bd47-8f52bdcb7197   1Gi        RWO            standard       10m
```

## Content of file

### node 1

```json
{'local time': '2021-10-04 20:02:42.285901+03:00', 'utc time': '2021-10-04 17:02:42.285949'}{'local time': '2021-10-04 20:02:42.642223+03:00', 'utc time': '2021-10-04 17:02:42.642297'}
```

### node 2

```json
{'local time': '2021-10-04 20:02:42.844341+03:00', 'utc time': '2021-10-04 17:02:42.844389'}
```

### node 3

```json
{'local time': '2021-10-04 20:02:40.996374+03:00', 'utc time': '2021-10-04 17:02:40.996422'}
```

They are different, since every replica has a separate volume mounted to it, thus
    application inside each pod writes to his own replica when load balancer forwards
    request to it. To prevent it we would need to attach the same volume to all 
    nodes.

## Pod management policy

The default policy for statefulset is Ordered update, when the pods terminate and 
recreate with new configuration sequentially. In our case we do not need that, so
we set this policy to parallel. In this case pods recreate in parallel, not depending
on other.